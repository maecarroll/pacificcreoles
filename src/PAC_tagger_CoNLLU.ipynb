{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import Elan2Str\n",
    "import os\n",
    "import re\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger:\n",
    "\n",
    "The pos tagger first looks up words from the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence, dic):\n",
    "    tagged_sentence = []\n",
    "    for word in sentence:\n",
    "        \n",
    "        if word in dic: \n",
    "            if len(dic[word]) == 1:\n",
    "                tag = list(dic[word])[0]                        #converts to list to remove set braces from output\n",
    "            else:\n",
    "                tag = pos_guess_fromset([word,list(dic[word])])\n",
    "        \n",
    "        else:\n",
    "            tag = pos_guess_unknown(word)\n",
    "        tagged_sentence.append({'word': word, \"pos\": tag})\n",
    "        \n",
    "    return(tagged_sentence) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If there are multiple options guess based on a specified list (sepcified below), OR\n",
    "- If its possibly a verb, there is some morphological guessing - this needs expanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_guess_fromset(word):                    #word here is list with (word, {pos*})\n",
    "    \n",
    "    rank = dict()\n",
    "    for value in word[1]:\n",
    "        rank[value] = freqdic[value]\n",
    "    rank = sorted(rank, key=rank.get, reverse=True)    \n",
    "    tag = rank[0]\n",
    "    \n",
    "    if 'VERB' in word[1]:\n",
    "        if re.match(\".+[eiu]m\", word[0]):                        # Guesses verb based on transitive morphology\n",
    "            tag = 'VERB'\n",
    "        elif re.match(\"^(.*)(doan|aot|raon|bak|ap)$\", word[0]):  # Guesses verb based on directional\n",
    "            tag = 'VERB'\n",
    "        elif word[0] in ('gat'):\n",
    "            tag = 'VERB'\n",
    "\n",
    "    #Other morphological guesses are possible here\n",
    "\n",
    "    return(tag)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, a guess is made based on orthography and morphology\n",
    "- Finally, if no idea, we call it a noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_guess_unknown(word):                            # Do a rule based guess function\n",
    "    \n",
    "    if word.istitle():                                  # if with caps we assume PROPN\n",
    "        tag = 'PROPN'\n",
    "    elif not re.match('[a-z]+', word):                  #if not some letters, its puctuation\n",
    "        tag = 'PUNCT'\n",
    "    elif re.match(\".*[0-9]+.*\", word):                  # if contains numbers assume numeral (data shouldn't contain numbers)\n",
    "        tag = 'NUM'\n",
    "    elif re.match('^a+$', word):                        # if a string of A's is assumes its hestiation\n",
    "        tag = '<hes>'\n",
    "    elif re.match('^ah$', word):                        # if a string of A's is assumes its hestiation\n",
    "        tag = '<hes>'\n",
    "    elif re.match(\".+[eiu]m\", word):                    # Guesses verb based on transitive morphology\n",
    "        tag = 'VERB'       \n",
    "    elif re.match(\"^(.*)(doan|aot|raon|bak|ap)$\", word):  #guesses verb based on directional\n",
    "        tag = 'VERB'\n",
    "    elif re.match(\"b.long\", word):                      #common variants for blong\n",
    "        tag = 'ADP'\n",
    "    else:\n",
    "        tag = 'NOUN'        \n",
    "    \n",
    "    return(tag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma Tagger:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes basic inflectional morphology and looks up in dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tag(sentence, dic):   #sentence is a tokenised list of lists with pos_tag\n",
    "    for word in sentence:\n",
    "        wordform = word['word']\n",
    "        \n",
    "        if re.match(\".+fala$\", wordform) and word['pos'] == ('ADJ' or 'NUM'):           #fala suffix\n",
    "            if wordform[0:-4] + 'f' in dic :        #checks for allomorphy      X-fala vs Xf-ala  \n",
    "                wordform = wordform + 'f'\n",
    "            else: \n",
    "                wordform = wordform[0:0-4]\n",
    "        \n",
    "        redup = re.compile(r\"^(.*)\\1$\")            #whole word reduplication\n",
    "        if re.match(redup, wordform) and word['pos'] == ('ADJ' or 'VERB'):\n",
    "            wordform = re.match(redup, wordform).group(1) \n",
    "        \n",
    "        if word['pos'] == 'VERB':\n",
    "            directional = re.compile(r\"^(.*)(doan|aot|raon|bak|ap)$\")\n",
    "            if re.match(directional, wordform):\n",
    "                if re.match(directional, wordform).group(1) in dic:\n",
    "                    wordform = (re.match(directional, wordform).group(1))\n",
    "        \n",
    "        if word['pos'] == '<hes>' or word['pos'] == 'PUNCT':\n",
    "            wordform = '_'\n",
    "        word.update({'lemma' : wordform})\n",
    "    return(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Parser:\n",
    "\n",
    "- Based on three functions slightly arbitrarily \n",
    "    - Works out NP structures (nominal dependency) \n",
    "    - Finds the verbs and their non-argument dependents (verbal dependency)\n",
    "    - Marks arguments to verb heads (head to head)\n",
    "    - Marks all elements to the roots (head to head)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nominal deps:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominal_dependency(sentence):\n",
    "    \n",
    "    pron_head_list = []\n",
    "    noun_head_list = []\n",
    "    nom_head_list = []\n",
    "    propn_head_list = []\n",
    "    \n",
    "    for i, word in enumerate(sentence):                          # HEAD TAGGER - finds nominal heads\n",
    "        if word['pos'] in ('PRON'):                             #Mark pronouns and propoer nouns as nominal heads to find their head later\n",
    "            word.update({'head': 'local_head'})\n",
    "            pron_head_list.append(i)\n",
    "            nom_head_list.append(i)\n",
    "        elif word['pos'] in ('PROPN', 'NOUN'):\n",
    "            try:                                                    #checks if final word in sentence\n",
    "                sentence[i+1]\n",
    "            except:\n",
    "                word.update({'head': 'local_head'})\n",
    "                nom_head_list.append(i)\n",
    "                if word['pos'] == 'NOUN':\n",
    "                    noun_head_list.append(i)\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                if word['pos'] == 'PROPN':                     \n",
    "                    if not sentence[i+1]['pos'] == 'PROPN':          #check if N+N* construction and only tag final noun\n",
    "                        word.update({'head': 'local_head'})\n",
    "                        propn_head_list.append(i)\n",
    "                        nom_head_list.append(i)            \n",
    "                \n",
    "                if word['pos'] == 'NOUN':                     \n",
    "                    if not sentence[i+1]['pos'] == 'NOUN':          #check if N+N* construction and only tag final noun\n",
    "                        word.update({'head': 'local_head'})\n",
    "                        noun_head_list.append(i)\n",
    "                        nom_head_list.append(i)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    for head in pron_head_list:                                            # pronoun headed NPs which are significantly more restricted\n",
    "        toggle = 0\n",
    "        for i, word in enumerate(sentence[head+1:head+2]):\n",
    "            if word['head'] == '_':                                      #checks to see if word has been annotated already\n",
    "                if i < 1: \n",
    "                    if word['pos'] == 'num':\n",
    "                        word.update({'head': head + 1, 'deprel': 'nummod'})\n",
    "                        toggle = 1\n",
    "                    elif word['pos'] == 'det':\n",
    "                        word.update({'head': head + 1, 'deprel': 'det'})\n",
    "                if i < 2 and toggle == 1:\n",
    "                    if word['pos'] == 'det':\n",
    "                        word.update({'head': head + 1, 'deprel': 'det'})\n",
    "    \n",
    "    \n",
    "    if propn_head_list != []:                                       # work out propn chains\n",
    "        for i, word in enumerate(sentence):\n",
    "            nexthead = min([k for k in propn_head_list if k > i], default = 'local_head') \n",
    "            if type(nexthead) == int:\n",
    "                nexthead += 1\n",
    "            if word['pos'] == 'PROPN' and word['head'] == '_':\n",
    "                word.update({'head': nexthead, 'deprel': 'flat'})\n",
    "    \n",
    "    if noun_head_list != []:                                            # NP depedents - go to closest \n",
    "        for i, word in enumerate(sentence):\n",
    "            nexthead = min([k for k in noun_head_list if k > i], default = 'local_head') \n",
    "            priorhead = max([k for k in noun_head_list if k < i], default = 'local_head')\n",
    "            if type(nexthead) == int:\n",
    "                nexthead += 1\n",
    "            if type(priorhead) == int:\n",
    "                priorhead += 1\n",
    "            if word['head'] == '_':\n",
    "                if word['pos'] == 'ADJ':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'amod'})\n",
    "                elif word['pos'] == 'NUM':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'nummod'})\n",
    "                elif word['pos'] == 'NOUN':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'nmod'})\n",
    "                elif word['pos'] == 'DET':\n",
    "                    word.update({'head': priorhead, 'deprel' : 'det'})\n",
    "\n",
    "\n",
    "    if nom_head_list != []:                                                                 #adding adpoisitions \n",
    "        for i, word in enumerate(sentence):\n",
    "            if word['pos'] == 'ADP':\n",
    "                nexthead = min([k for k in nom_head_list if k > i], default = '_') \n",
    "                if type(nexthead) == int:\n",
    "            \n",
    "                    word.update({'head': nexthead + 1, 'deprel' : 'case'})\n",
    "                    try:\n",
    "                        sentence[nexthead]\n",
    "                    except:\n",
    "                        word.update({'head': 'fail'})\n",
    "                    else:\n",
    "                        sentence[nexthead].update({'feats' : 'loc'})    \n",
    "        \n",
    "                                    \n",
    "\n",
    "\n",
    "    return(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbal_dependency(sentence):\n",
    "    \n",
    "    svclist = ['splitem', 'brekem', 'klinem', 'blokem', 'spolem', 'hipimap', 'fasem', 'flatem', 'finisim', 'panisim', 'meksave', 'haed', 'raf', 'stil', 'taet', 'redi']\n",
    "    auxlist = ['save', 'sae', 'stap', 'kanduit' , 'wantem' , 'kam', 'go' , 'mas', 'bin', 'jas' ,'sud']\n",
    "    postverbalmodifiers = ['nating', 'finis', 'yet', 'gogo', 'mo']\n",
    "    \n",
    "    verb_list = []\n",
    "\n",
    "    \n",
    "    for i,word in enumerate(sentence):                                     # Mark out verbs\n",
    "        \n",
    "\n",
    "        if word['pos'] == 'VERB':                                           #SVCs\n",
    "            if sentence[i-1]['pos'] =='VERB':\n",
    "                if word['word'] in svclist:\n",
    "                    word.update({'head': i, 'deprel': 'compound:svc'})\n",
    "                else:\n",
    "                    verb_list.append(i)\n",
    "                    word.update({'head': 'verb_head'})\n",
    "            else:\n",
    "                verb_list.append(i)\n",
    "                word.update({'head': 'verb_head'})\n",
    "    \n",
    "        if word['word'] in auxlist:                                        # multifucntion auxiliaries\n",
    "            try:\n",
    "                sentence[i+1]\n",
    "            except:\n",
    "                verb_list.append(i)\n",
    "                word.update({'head': 'verb_head'})\n",
    "            else:\n",
    "                if sentence[i+1]['pos'] == 'VERB':\n",
    "                    word.update({'pos': 'AUX', 'head': i+1, 'deprel': 'aux'})      \n",
    "                else:\n",
    "                    word.update({'head': 'verb_head'})\n",
    "        \n",
    "        \n",
    "        elif word['word'] in postverbalmodifiers:\n",
    "            try:\n",
    "                sentence[i-1]\n",
    "            except:\n",
    "                pass\n",
    "            else:        \n",
    "                if sentence[i-1]['pos'] == 'VERB':\n",
    "                    word.update({'pos': 'AUX', 'head': i-1, 'deprel': 'aux'})           #needs to be a bit more sophisticated  \n",
    "        \n",
    "\n",
    "    \n",
    "    for i,word in enumerate(sentence):                                    \n",
    "        \n",
    "        if word['pos'] in ('AUX', 'PART'):                              # Mark auxiliaries and particles as aux dependents on the following verb\n",
    "        \n",
    "        #if there is a verb following, it is the head, if there is no verb it is the verb\n",
    "            \n",
    "            if word['word'] in ('i', 'oli'):\n",
    "                nexthead = min([k for k in verb_list if k > i], default = 'verb_head') \n",
    "                if type(nexthead) == int:\n",
    "                    nexthead += 1\n",
    "                try:\n",
    "                    sentence[i+1]\n",
    "                except:\n",
    "                    word.update({'head': 'verb_head'})    \n",
    "                else:\n",
    "                    if sentence[i+1]['pos'] in ('VERB', 'AUX', 'PART'):\n",
    "                        word.update({'head': nexthead, 'deprel': 'aux'})\n",
    "                    else:\n",
    "                        word.update({'head': 'verb_head'})\n",
    "            \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                nexthead = min([k for k in verb_list if k > i], default = max(verb_list, default = 0)) + 1\n",
    "                word.update({'head': nexthead, 'deprel': 'aux'})\n",
    "                \n",
    "        elif word['pos'] == 'ADV':                                      # Adverbs are assigned their closest verb as head (needs to do better but a good start)\n",
    "                closesthead = min(verb_list, key=lambda x:abs(x-i), default = 0) + 1\n",
    "                word.update({'head': closesthead, 'deprel' : 'advmod'})   \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head to Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_to_head_depedency(sentence):\n",
    "    \n",
    "    nominal_tags =['ADJ', 'NOUN', 'NUM']\n",
    "    \n",
    "    local_heads = []\n",
    "    verbal_heads = []\n",
    "    all_heads = []   \n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(sentence):                      # Make lists of heads\n",
    "\n",
    "        if word['head'] == 'local_head':\n",
    "            local_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "        elif word['head'] == 'verb_head':\n",
    "            verbal_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for head in all_heads:                                # NPs that follow a PP modift a prior NP (check this for verbs like go)\n",
    "        priorhead = max([k for k in all_heads if k < head], default = 'default') \n",
    "        if type(priorhead) == int:\n",
    "            if 'loc' in sentence[head]['feats']:\n",
    "                if sentence[priorhead]['head'] == 'local_head':\n",
    "                    sentence[head].update({'head': priorhead + 1, 'deprel': 'nmod'})\n",
    "\n",
    "                if sentence[priorhead]['pos'] in ('VERB', 'PART', 'AUX'):\n",
    "                    sentence[head].update({'head': priorhead + 1, 'deprel': 'obl'})\n",
    "                    \n",
    "                    \n",
    "    \n",
    "    for i,head in enumerate(all_heads):                       #possessive pronouns that immediately precede a NP are said to possess that NP\n",
    "        if sentence[head]['pos'] == 'PRON':\n",
    "            try:\n",
    "                sentence[head+1]\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if sentence[head + 1]['pos'] in (nominal_tags):                \n",
    "                    sentence[head].update({'head' : head + 2, 'deprel' : 'nmod:pos'})\n",
    "         \n",
    "                                                                 \n",
    "    \n",
    "    if len(local_heads) > 0 and len(verbal_heads) > 0:          #Marks subjects of all verbs and objects of transitive verbs\n",
    "        for i,head in enumerate(verbal_heads):\n",
    "            subject = max([k for k in local_heads if k < head], default = 'no np')\n",
    "            if type(subject) == int:\n",
    "                sentence[subject].update({'head': head + 1, 'deprel': 'nsubj'})\n",
    "            if re.match(\"^(.+[eiu]m)(|doan$|aot$|raon$|bak$|ap$)\", sentence[head]['word']):                    # Guesses verb based on transitive morphology    \n",
    "                object = min([k for k in local_heads if k > head], default = 'def')\n",
    "                if type(object) == int:\n",
    "                    sentence[object].update({'head' : head + 1, 'deprel' : 'obj'})\n",
    "            \n",
    "            if sentence[head]['word'] == 'i':\n",
    "                object = min([k for k in local_heads if k > head], default = 'def')\n",
    "                if type(object) == int:\n",
    "                    sentence[object].update({'head' : head + 1, 'deprel' : 'obj'})\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,head in enumerate(verbal_heads):                          # relative clauses\n",
    "        if i < 1:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = verbal_heads[i-1]\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word['word'] == 'we':\n",
    " \n",
    "                for word2 in sentence[start:head]:\n",
    "                    if word2['deprel'] in ('local_head', 'nsubj', 'obj'):\n",
    "                        sentence[head]['head'] = sentence.index(word2) + 1\n",
    "                        sentence[head]['deprel'] = 'acl:relcl'      #mark as dependent on the first NP\n",
    "                        break\n",
    "        \n",
    "                    else:\n",
    "                        sentence[head]['head'] = 'headless_relative'       #If not mark as a headless relative clause\n",
    "                        break\n",
    "                break      \n",
    "            \n",
    "            \n",
    "    if len(verbal_heads) > 1:        \n",
    "        toggle = 0\n",
    "        relativizer = 0\n",
    "        for i,word in enumerate(sentence):                                               # Adverbial clauses\n",
    "            \n",
    "            if word['word'] in ('se', 'blong', 'long', 'blo', 'bl'):\n",
    "                \n",
    "                toggle = 1\n",
    "                relativizer = i \n",
    "                \n",
    "            elif toggle == 1:\n",
    "                if word['head'] == 'verb_head':         \n",
    "                    sentence[relativizer]['head'] = i + 1\n",
    "                    sentence[relativizer]['deprel'] = 'mark'\n",
    "                    word['head'] = 'link_to_root'\n",
    "                    word['deprel'] = 'advcl'\n",
    "                    break\n",
    "\n",
    "\n",
    "## Recalculate the heads:\n",
    "\n",
    "    local_heads = []\n",
    "    verbal_heads = []\n",
    "    all_heads = []   \n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(sentence):                      # Make lists of heads\n",
    "\n",
    "        if word['head'] == 'local_head':\n",
    "            local_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "        elif word['head'] == 'verb_head':\n",
    "            verbal_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "\n",
    "\n",
    "\n",
    "## Find the root\n",
    "    \n",
    "    root = 0\n",
    "    \n",
    "    if len(all_heads) > 0:\n",
    "                    \n",
    "        if len(all_heads) == 1:                                    # If there is just one possible head, it is the root\n",
    "            sentence[all_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "            root = all_heads[0]\n",
    "\n",
    "        elif len(verbal_heads) == 1:                                 # If there is just one verb head, it is the root (could be better)\n",
    "            sentence[verbal_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "            root = verbal_heads[0]\n",
    "\n",
    "        elif len(verbal_heads) > 1: \n",
    "            sentence[verbal_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "            root = verbal_heads[0]\n",
    "\n",
    "        elif len(verbal_heads) < 1:\n",
    "            sentence[all_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "            root = all_heads[0]\n",
    "            \n",
    "\n",
    "\n",
    "    root += 1\n",
    "    \n",
    "    \n",
    "# Mark remaining elements as obliques or adverbial clauses\n",
    "\n",
    "    for word in sentence:\n",
    "\n",
    "        if word['head']  == 'headless_relative':\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'obl'\n",
    "        elif word['head'] in ('verb_head', 'link_to_root'):\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'advcl'\n",
    "        elif word['head'] in ('local_head', '_'):\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'adv'\n",
    "        \n",
    "        if word['pos'] == '<hes>':\n",
    "            word['deprel'] = 'dep'\n",
    "        \n",
    "\n",
    "    \n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser wrapper\n",
    "\n",
    "- Marks everything as empty and then applies the three dependency functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(sentence):\n",
    "\n",
    "    for word in sentence:\n",
    "        if 'head' not in word:\n",
    "            word.update({'head' : '_'})\n",
    "        if 'deprel' not in word:\n",
    "            word.update({'deprel' : '_'})\n",
    "        if 'feats' not in word:\n",
    "            word.update({'feats' : '_'})\n",
    "\n",
    "    sentence = nominal_dependency(sentence)\n",
    "\n",
    "    sentence = verbal_dependency(sentence)\n",
    "    \n",
    "    sentence = head_to_head_depedency(sentence)\n",
    "    \n",
    "    \n",
    "    return(sentence)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_stripper(sentence):    \n",
    "    \n",
    "    sentence = sentence.strip()                                 # Strip here removes final stops and spaces \n",
    "    sentence = sentence.strip('.')\n",
    "    sentence = sentence.strip('# text=')                             # removes text tag\n",
    "    sentence = re.sub(\"\\s*\\%.*?\\%\\s*\", \"\", sentence)                # Removes tags marked with '%'\n",
    "    sentence = re.sub(\"\\s*\\(+.*?\\)+\\s*\", \"\", sentence)              # Removes parathetic comments\n",
    "    sentence = re.sub(\"\\s*\\<+.*?\\>+\\s*\", \"\", sentence)              # Removes hesitations\n",
    "    \n",
    "    return(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to run:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose data folder, dictionary file and output filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/BIS_round1_train/\"               #Folder with input eafs\n",
    "\n",
    "dicdir = \"../data/BIS_Dictionary_3col.replaced.csv\" #Location of dictionary\n",
    "\n",
    "output_file = 'BIS_round1_Tree.conllu'              #output tagged data for just running the tools\n",
    "\n",
    "test_output_file = 'BIS_round1_testing_Tree.conllu' #output tagged data when testing\n",
    "\n",
    "test_input_file = '../data/test/MC.conllu'          #Test data\n",
    "\n",
    "tier_name = \"BIS\"                                   #Tier name for eaf extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_dictionary = {}\n",
    "\n",
    "with open(dicdir) as f:\n",
    "    for line in f:\n",
    "        (k, v1, v2) = line.split(',')\n",
    "        if k in lang_dictionary:\n",
    "            lang_dictionary[k].add(v1)\n",
    "        else:\n",
    "            lang_dictionary[k] = {v1}\n",
    "\n",
    "freqdic = {'NOUN': 12, 'VERB' : 11, 'PRON' : 10, 'ADP' : 13, 'PART' : 13, 'ADV' : 4, 'ADJ' : 6, 'NUM': 5, 'CCONJ': 4, 'SCONJ': 4, 'DET': 3, 'INTJ': 2, 'PROPN': 1, 'AUX': 14}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over elan files, extract text and tag text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = \"\"                                                               # output string starts empty\n",
    "\n",
    "for file in os.listdir(datadir):\n",
    "    filepath = datadir + file\n",
    "    text = Elan2Str.elan2str(filepath, tier_name)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    \n",
    "    counter = 0\n",
    "    for sentence in sentences:\n",
    "        counter += 1\n",
    "        metadata = '# ' + 'sent_id = ' + str(file) + '.' + str(counter) + '\\n' + '# ' + 'text = ' + str(sentence) + '\\n' #maybe make these variables \n",
    "        tree += metadata\n",
    "        word_counter = 0\n",
    "\n",
    "        sentence = sent_stripper(sentence)\n",
    "\n",
    "        if not re.match(\"^\\s*[\\,\\.\\(\\)\\{\\}\\[\\]]*\\s*$\", sentence):       # Ignores sentences which are empty or punctuation\n",
    "        \n",
    "            token_sentence = nltk.word_tokenize(sentence)                           # Tokenise\n",
    "            pos_tagged_sentence = pos_tag(token_sentence, lang_dictionary)          # POS tag\n",
    "            lemma_tagged_sentence = lemma_tag(pos_tagged_sentence, lang_dictionary) # Lemmatise\n",
    "            parsed_sentence = parse(lemma_tagged_sentence)                          # Depedency parser\n",
    "       \n",
    "            for word in parsed_sentence:                                            # Appends each word to the 'tree' for output\n",
    "                word_counter += 1\n",
    "                tree += str(word_counter) + '\\t' + word['word'] + '\\t' + word['lemma'] + '\\t' + word['pos'] + '\\t' + '_' + '\\t' +  '_' + '\\t' + str(word['head']) + '\\t' + word['deprel'] + '\\t' + '_' + '\\t' + '_' + '\\n'    \n",
    "        \n",
    "        tree += '\\n'\n",
    "        \n",
    "        \n",
    "with open(output_file, 'w') as output:\n",
    "    output.write(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The follow cell takes your test file and extracts the texts from each line and builds a conllu tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = \"\"                                                               # output string starts empty\n",
    "\n",
    "with open(test_input_file) as file:\n",
    "\n",
    "    for line in file:\n",
    "\n",
    "        if line.startswith('# sent_id'):\n",
    "            metadata = line\n",
    "            tree += metadata \n",
    "            \n",
    "        if line.startswith('# text'):\n",
    "            sentence = line \n",
    "            word_counter = 0\n",
    "            tree += sentence\n",
    "\n",
    "            sentence = sent_stripper(sentence)\n",
    "            \n",
    "            if not re.match(\"^\\s*[\\,\\.\\(\\)\\{\\}\\[\\]]*\\s*$\", sentence):       # Ignores sentences which are empty or punctuation\n",
    "                token_sentence = nltk.word_tokenize(sentence)                           # Tokenise\n",
    "                pos_tagged_sentence = pos_tag(token_sentence, lang_dictionary)          # POS tag\n",
    "                lemma_tagged_sentence = lemma_tag(pos_tagged_sentence, lang_dictionary) # Lemmatise\n",
    "                parsed_sentence = parse(lemma_tagged_sentence)                          # Depedency parser\n",
    "        \n",
    "                for word in parsed_sentence:                                            # Appends each word to the 'tree' for output\n",
    "                    word_counter += 1\n",
    "                    tree += str(word_counter) + '\\t' + word['word'] + '\\t' + word['lemma'] + '\\t' + word['pos'] + '\\t' + '_' + '\\t' +  '_' + '\\t' + str(word['head']) + '\\t' + word['deprel'] + '\\t' + '_' + '\\t' + '_' + '\\n'    \n",
    "            \n",
    "            tree += '\\n'\n",
    "        \n",
    "        \n",
    "with open(test_output_file, 'w') as output:\n",
    "    output.write(tree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following compares the manually annotated testing data to the final data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of correct lemmas: 78.71\n",
      "Percentage of correct POS tags: 77.51\n",
      "Percentage of correct heads: 56.63\n",
      "Percentage of correct relation tags: 49.4\n",
      "Percentage of correct LAS: 44.18\n"
     ]
    }
   ],
   "source": [
    "with open(test_input_file ) as man:\n",
    "    with open(test_output_file) as test: \n",
    "        man_lemma = []\n",
    "        man_pos = []\n",
    "        man_head = []\n",
    "        man_rel = []\n",
    "        man_headrel = []\n",
    "        for line in man:\n",
    "            if line[:1].isdigit():\n",
    "                line_list = [item for item in line.split('\\t')]\n",
    "                man_lemma.append(line_list[2])\n",
    "                man_pos.append(line_list[3])\n",
    "                man_head.append(line_list[6])\n",
    "                man_rel.append(line_list[7])\n",
    "                man_headrel.append((line_list[6], line_list[7]))      \n",
    "        \n",
    "        test_lemma = []\n",
    "        test_pos = []\n",
    "        test_head = []\n",
    "        test_rel = []\n",
    "        test_headrel = []\n",
    "        for line in test:\n",
    "            if line[:1].isdigit():\n",
    "                line_list = [item for item in line.split('\\t')]\n",
    "                test_lemma.append(line_list[2])\n",
    "                test_pos.append(line_list[3])\n",
    "                test_head.append(line_list[6])\n",
    "                test_rel.append(line_list[7])\n",
    "                test_headrel.append((line_list[6], line_list[7]))\n",
    "                \n",
    "        lemma_count = len([x for x,y in zip(man_lemma,test_lemma) if x == y])\n",
    "        pos_count = len([x for x,y in zip(man_pos,test_pos) if x == y])\n",
    "        head_count = len([x for x,y in zip(man_head,test_head) if x == y])\n",
    "        rel_count = len([x for x,y in zip(man_rel,test_rel) if x == y])\n",
    "        headrel_count = len([x for x,y in zip(man_headrel,test_headrel) if x == y])\n",
    "        \n",
    "        total_tag = len(man_lemma)\n",
    "        \n",
    "               \n",
    "        lemma_percent = lemma_count / total_tag * 100\n",
    "        pos_percent = pos_count / total_tag * 100\n",
    "        head_percent = head_count / total_tag * 100\n",
    "        rel_percent = rel_count / total_tag * 100\n",
    "        headrel_percent = headrel_count / total_tag * 100\n",
    "        \n",
    "        print(\"Percentage of correct lemmas: \" + str(round(lemma_percent,2)))\n",
    "        print(\"Percentage of correct POS tags: \" + str(round(pos_percent,2)))\n",
    "        print(\"Percentage of correct heads: \" + str(round(head_percent, 2)))\n",
    "        print(\"Percentage of correct relation tags: \" + str(round(rel_percent,2)))\n",
    "        print(\"Percentage of correct LAS: \" + str(round(headrel_percent,2)))\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
