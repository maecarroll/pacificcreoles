{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import Elan2Str\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tag(sentence, dic):\n",
    "    tagged_sentence = []\n",
    "    for word in sentence:\n",
    "        \n",
    "        if word in dic: \n",
    "            if len(dic[word]) == 1:\n",
    "                tag = list(dic[word])[0]                        #converts to list to remove set braces from output\n",
    "            else:\n",
    "                tag = pos_guess_fromset([word,list(dic[word])])\n",
    "        \n",
    "        else:\n",
    "            tag = pos_guess_unknown(word)\n",
    "        tagged_sentence.append({'word': word, \"pos\": tag})\n",
    "        \n",
    "    return(tagged_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_guess_fromset(word):                    #word here is list with (word, {pos*})\n",
    "    \n",
    "    rank = dict()\n",
    "    for value in word[1]:\n",
    "        rank[value] = freqdic[value]\n",
    "    rank = sorted(rank, key=rank.get, reverse=True)    \n",
    "    tag = rank[0]\n",
    "    \n",
    "    if 'VERB' in word[1]:\n",
    "        if re.match(\".+[eiu]m\", word[0]):                    # Guesses verb based on transitive morphology\n",
    "            tag = 'VERB'\n",
    "        elif re.match(\"^(.*)(doan|aot|raon|bak|ap)$\", word[0]):  #guesses verb based on directional\n",
    "            tag = 'VERB'\n",
    "        elif word[0] in ('gat'):\n",
    "            tag = 'VERB'\n",
    "\n",
    "    #Other morphological guesses are possible here\n",
    "\n",
    "    return(tag)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_guess_unknown(word):                            # Do a rule based guess function\n",
    "    \n",
    "    if word.istitle():                                  # if with caps we assume PROPN\n",
    "        tag = 'PROPN'\n",
    "    elif not re.match('[a-z]+', word):                  #if not some letters, its puctuation\n",
    "        tag = 'PUNCT'\n",
    "    elif re.match(\".*[0-9]+.*\", word):                  # if contains numbers assume numeral (data shouldn't contain numbers)\n",
    "        tag = 'NUM'\n",
    "    elif re.match('^a+$', word):                        # if a string of A's is assumes its hestiation\n",
    "        tag = '<hes>'\n",
    "    elif re.match('^ah$', word):                        # if a string of A's is assumes its hestiation\n",
    "        tag = '<hes>'\n",
    "    elif re.match(\".+[eiu]m\", word):                    # Guesses verb based on transitive morphology\n",
    "        tag = 'VERB'       \n",
    "    elif re.match(\"^(.*)(doan|aot|raon|bak|ap)$\", word):  #guesses verb based on directional\n",
    "        tag = 'VERB'\n",
    "    elif re.match(\"b.long\", word):                      #common variants for blong\n",
    "        tag = 'ADP'\n",
    "    else:\n",
    "        tag = 'NOUN'        \n",
    "    \n",
    "    return(tag)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemma Tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_tag(sentence, dic):   #sentence is a tokenised list of lists with pos_tag\n",
    "    for word in sentence:\n",
    "        wordform = word['word']\n",
    "        \n",
    "        if re.match(\".+fala$\", wordform) and word['pos'] == ('ADJ' or 'NUM'):           #fala suffix\n",
    "            if wordform[0:-4] + 'f' in dic :        #checks for allomorphy      X-fala vs Xf-ala  \n",
    "                wordform = wordform + 'f'\n",
    "            else: \n",
    "                wordform = wordform[0:0-4]\n",
    "        \n",
    "        redup = re.compile(r\"^(.*)\\1$\")            #whole word reduplication\n",
    "        if re.match(redup, wordform) and word['pos'] == ('ADJ' or 'VERB'):\n",
    "            wordform = re.match(redup, wordform).group(1) \n",
    "        \n",
    "        if word['pos'] == 'VERB':\n",
    "            directional = re.compile(r\"^(.*)(doan|aot|raon|bak|ap)$\")\n",
    "            if re.match(directional, wordform):\n",
    "                if re.match(directional, wordform).group(1) in dic:\n",
    "                    wordform = (re.match(directional, wordform).group(1))\n",
    "        \n",
    "        if word['pos'] == '<hes>' or word['pos'] == 'PUNCT':\n",
    "            wordform = '_'\n",
    "        word.update({'lemma' : wordform})\n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Based Parser:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nominal deps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominal_dependency(sentence):\n",
    "    \n",
    "    pron_head_list = []\n",
    "    noun_head_list = []\n",
    "    nom_head_list = []\n",
    "    \n",
    "    for i, word in enumerate(sentence):                          # HEAD TAGGER - finds nominal heads\n",
    "        if word['pos'] in ('PRON'):                             #Mark pronouns and propoer nouns as nominal heads to find their head later\n",
    "            word.update({'head': 'local_head'})\n",
    "            pron_head_list.append(i)\n",
    "            nom_head_list.append(i)\n",
    "        elif word['pos'] in ('PROPN', 'NOUN'):\n",
    "            try:                                                    #checks if final word in sentence\n",
    "                sentence[i+1]\n",
    "            except:\n",
    "                word.update({'head': 'local_head'})\n",
    "                nom_head_list.append(i)\n",
    "                if word['pos'] == 'NOUN':\n",
    "                    noun_head_list.append(i)\n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                if word['pos'] == 'PROPN':\n",
    "                    lastpropn = i\n",
    "                    if sentence[i+1]['pos'] == 'PROPN':             #check if PROPN+PROPN* construction \n",
    "                        for j,word in enumerate(sentence[i:]):      #count how many PROPN\n",
    "                            if not word['pos'] == 'PROPN':\n",
    "                                lastpropn += j-1\n",
    "                                break\n",
    "                            elif word == sentence[-1]:\n",
    "                                lastpropn += j \n",
    "                    for propn in sentence[i:lastpropn]:              #Mark last PROPN as head and others as depedent\n",
    "                        if propn == sentence[lastpropn]:                       \n",
    "                            propn.update({'head': 'local_head'})\n",
    "                            nom_head_list.append(i)\n",
    "                        else:\n",
    "                            propn.update({'head': lastpropn + 1, 'deprel': 'flat'})\n",
    "                \n",
    "                \n",
    "                            \n",
    "                elif word['pos'] == 'NOUN':                     \n",
    "                    if not sentence[i+1]['pos'] == 'NOUN':          #check if N+N* construction and only tag final noun\n",
    "                        word.update({'head': 'local_head'})\n",
    "                        noun_head_list.append(i)\n",
    "                        nom_head_list.append(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for head in pron_head_list:\n",
    "        toggle = 0\n",
    "        for i, word in enumerate(sentence[head+1:head+2]):\n",
    "            if word['head'] == '_':                                      #checks to see if word has been annotated already\n",
    "                if i < 1: \n",
    "                    if word['pos'] == 'num':\n",
    "                        word.update({'head': head + 1, 'deprel': 'nummod'})\n",
    "                        toggle = 1\n",
    "                    elif word['pos'] == 'det':\n",
    "                        word.update({'head': head + 1, 'deprel': 'det'})\n",
    "                if i < 2 and toggle == 1:\n",
    "                    if word['pos'] == 'det':\n",
    "                        word.update({'head': head + 1, 'deprel': 'det'})\n",
    "                    \n",
    "    \n",
    "    if noun_head_list != []:\n",
    "        for i, word in enumerate(sentence):\n",
    "            nexthead = min([k for k in noun_head_list if k > i], default = 'local_head') \n",
    "            priorhead = max([k for k in noun_head_list if k < i], default = 'local_head')\n",
    "            if type(nexthead) == int:\n",
    "                nexthead += 1\n",
    "            if type(priorhead) == int:\n",
    "                priorhead += 1\n",
    "            if word['head'] == '_':\n",
    "                if word['pos'] == 'ADJ':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'amod'})\n",
    "                elif word['pos'] == 'NUM':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'nummod'})\n",
    "                elif word['pos'] == 'NOUN':\n",
    "                    word.update({'head': nexthead, 'deprel' : 'nmod'})\n",
    "                elif word['pos'] == 'DET':\n",
    "                    word.update({'head': priorhead, 'deprel' : 'det'})\n",
    "\n",
    "\n",
    "    if nom_head_list != []:                                                                 #adding adpoisitions \n",
    "        for i, word in enumerate(sentence):\n",
    "            if word['pos'] == 'ADP':\n",
    "                nexthead = min([k for k in nom_head_list if k > i], default = '_') \n",
    "                if type(nexthead) == int:\n",
    "            \n",
    "                    word.update({'head': nexthead + 1, 'deprel' : 'case'})\n",
    "                    try:\n",
    "                        sentence[nexthead]\n",
    "                    except:\n",
    "                        word.update({'head': 'fail'})\n",
    "                    else:\n",
    "                        sentence[nexthead].update({'feats' : 'loc'})    \n",
    "        \n",
    "                                    \n",
    "\n",
    "\n",
    "    return(sentence)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbal deps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aux, part and adv to appropriate verb\n",
    "\n",
    "\n",
    "- Multifunction verbs\n",
    "    - Lexical listing\n",
    "\n",
    "\n",
    "\n",
    "- Complex verbs\n",
    "    - V + V sequences\n",
    "\n",
    "\n",
    "\n",
    "- clause final verbs and other speech act\n",
    "\n",
    "Multifunctionality of any syntax related elements\n",
    "\n",
    "aux - preverbal\n",
    "save\n",
    "sae\n",
    "stap\n",
    "kanduit\n",
    "wantem\n",
    "kam\n",
    "go\n",
    "mas\n",
    "bin\n",
    "jas\n",
    "sud\n",
    "\n",
    "post-verbal:\n",
    "nating\n",
    "finis (post verbal or after object)\n",
    "yet \n",
    "gogo \n",
    "mo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbal_dependency(sentence):\n",
    "    \n",
    "    svclist = ['splitem', 'brekem', 'klinem', 'blokem', 'spolem', 'hipimap', 'fasem', 'flatem', 'finisim', 'panisim', 'meksave', 'haed', 'raf', 'stil', 'taet', 'redi']\n",
    "    auxlist = ['save', 'sae', 'stap', 'kanduit' , 'wantem' , 'kam', 'go' , 'mas', 'bin', 'jas' ,'sud']\n",
    "    postverbalmodifiers = ['nating', 'finis', 'yet', 'gogo', 'mo']\n",
    "    \n",
    "    verb_list = []\n",
    "\n",
    "    \n",
    "    for i,word in enumerate(sentence):                                     # Mark out verbs\n",
    "        \n",
    "\n",
    "        if word['pos'] == 'VERB':                                           #SVCs\n",
    "            if sentence[i-1]['pos'] =='VERB':\n",
    "                if word['word'] in svclist:\n",
    "                    word.update({'head': i, 'deprel': 'compound:svc'})\n",
    "                else:\n",
    "                    verb_list.append(i)\n",
    "                    word.update({'head': 'verb_head'})\n",
    "            else:\n",
    "                verb_list.append(i)\n",
    "                word.update({'head': 'verb_head'})\n",
    "    \n",
    "        if word['word'] in auxlist:                                        # multifucntion auxiliaries\n",
    "            try:\n",
    "                sentence[i+1]\n",
    "            except:\n",
    "                verb_list.append(i)\n",
    "                word.update({'head': 'verb_head'})\n",
    "            else:\n",
    "                if sentence[i+1]['pos'] == 'VERB':\n",
    "                    word.update({'pos': 'AUX', 'head': i+1, 'deprel': 'aux'})      \n",
    "                else:\n",
    "                    word.update({'head': 'verb_head'})\n",
    "        \n",
    "        \n",
    "        elif word['word'] in postverbalmodifiers:\n",
    "            try:\n",
    "                sentence[i-1]\n",
    "            except:\n",
    "                pass\n",
    "            else:        \n",
    "                if sentence[i-1]['pos'] == 'VERB':\n",
    "                    word.update({'pos': 'AUX', 'head': i-1, 'deprel': 'aux'})           #needs to be a bit more sophisticated  \n",
    "        \n",
    "\n",
    "    \n",
    "    for i,word in enumerate(sentence):                                    \n",
    "        \n",
    "        if word['pos'] in ('AUX', 'PART'):                              # Mark auxiliaries and particles as aux dependents on the following verb\n",
    "        \n",
    "        #if there is a verb following, it is the head, if there is no verb it is the verb\n",
    "            \n",
    "            if word['word'] in ('i', 'oli'):\n",
    "                nexthead = min([k for k in verb_list if k > i], default = 'verb_head') \n",
    "                if type(nexthead) == int:\n",
    "                    nexthead += 1\n",
    "                try:\n",
    "                    sentence[i+1]\n",
    "                except:\n",
    "                    word.update({'head': 'verb_head'})    \n",
    "                else:\n",
    "                    if sentence[i+1]['pos'] in ('VERB', 'AUX', 'PART'):\n",
    "                        word.update({'head': nexthead, 'deprel': 'aux'})\n",
    "                    else:\n",
    "                        word.update({'head': 'verb_head'})\n",
    "            \n",
    "                \n",
    "                \n",
    "            else:\n",
    "                nexthead = min([k for k in verb_list if k > i], default = max(verb_list, default = 0)) + 1\n",
    "                word.update({'head': nexthead, 'deprel': 'aux'})\n",
    "                \n",
    "        elif word['pos'] == 'ADV':                                      # Adverbs are assigned their closest verb as head (needs to do better but a good start)\n",
    "                closesthead = min(verb_list, key=lambda x:abs(x-i), default = 0) + 1\n",
    "                word.update({'head': closesthead, 'deprel' : 'advmod'})   \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(sentence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### head to head\n",
    "\n",
    "local head to adposition head\n",
    "\n",
    "adposition head to verbal head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_to_head_depedency(sentence):\n",
    "    \n",
    "    nominal_tags =['ADJ', 'NOUN', 'NUM']\n",
    "    \n",
    "    local_heads = []\n",
    "    verbal_heads = []\n",
    "    all_heads = []   \n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(sentence):                      # Make lists of heads\n",
    "\n",
    "        if word['head'] == 'local_head':\n",
    "            local_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "        elif word['head'] == 'verb_head':\n",
    "            verbal_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for head in local_heads:                                # NPs that follow a PP modift a prior NP (check this for verbs like go)\n",
    "        priorhead = max([k for k in local_heads if k < head], default = 'default') \n",
    "        if type(priorhead) == int:\n",
    "            if 'loc' in sentence[head]['feats']:\n",
    "                sentence[head].update({'head': priorhead + 1, 'deprel': 'nmod'})\n",
    "\n",
    "    \n",
    "    for i,head in enumerate(all_heads):                       #possessive pronouns that immediately precede a NP are said to possess that NP\n",
    "        if sentence[head]['pos'] == 'PRON':\n",
    "            try:\n",
    "                sentence[head+1]\n",
    "            except:\n",
    "                pass\n",
    "            else:\n",
    "                if sentence[head + 1]['pos'] in (nominal_tags):                \n",
    "                    sentence[head].update({'head' : head + 1, 'deprel' : 'nmod:pos'})\n",
    "         \n",
    "                                                                 \n",
    "    \n",
    "    if len(local_heads) > 0 and len(verbal_heads) > 0:          #Marks subjects of all verbs and objects of transitive verbs\n",
    "        for i,head in enumerate(verbal_heads):\n",
    "            subject = max([k for k in local_heads if k < head], default = 'no np')\n",
    "            if type(subject) == int:\n",
    "                sentence[subject].update({'head': head + 1, 'deprel': 'nsubj'})\n",
    "            if re.match(\"^(.+[eiu]m)(|doan$|aot$|raon$|bak$|ap$)\", sentence[head]['word']):                    # Guesses verb based on transitive morphology    \n",
    "                object = min([k for k in local_heads if k > head], default = 'def')\n",
    "                if type(object) == int:\n",
    "                    sentence[object].update({'head' : head + 1, 'deprel' : 'obj'})\n",
    "            \n",
    "            if sentence[head]['word'] == 'i':\n",
    "                object = min([k for k in local_heads if k > head], default = 'def')\n",
    "                if type(object) == int:\n",
    "                    sentence[object].update({'head' : head + 1, 'deprel' : 'obj'})\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,head in enumerate(verbal_heads):                          # relative clauses\n",
    "        if i < 1:\n",
    "            start = 0\n",
    "        else:\n",
    "            start = verbal_heads[i-1]\n",
    "        \n",
    "        for word in sentence:\n",
    "            if word['word'] == 'we':\n",
    " \n",
    "                for word2 in sentence[start:head]:\n",
    "                    if word2['deprel'] in ('local_head', 'nsubj', 'obj'):\n",
    "                        sentence[head]['head'] = sentence.index(word2) + 1\n",
    "                        sentence[head]['deprel'] = 'acl:relcl'      #mark as dependent on the first NP\n",
    "                        break\n",
    "        \n",
    "                    else:\n",
    "                        sentence[head]['head'] = 'headless_relative'       #If not mark as a headless relative clause\n",
    "                        break\n",
    "                break      \n",
    "            \n",
    "            \n",
    "    if len(verbal_heads) > 1:        \n",
    "        toggle = 0\n",
    "        relativizer = 0\n",
    "        for i,word in enumerate(sentence):                                               # Adverbial clauses\n",
    "            \n",
    "            if word['word'] in ('se', 'blong', 'long', 'blo', 'bl'):\n",
    "                \n",
    "                toggle = 1\n",
    "                relativizer = i \n",
    "                \n",
    "            elif toggle == 1:\n",
    "                if word['head'] == 'verb_head':         \n",
    "                    sentence[relativizer]['head'] = i + 1\n",
    "                    sentence[relativizer]['deprel'] = 'mark'\n",
    "                    word['head'] = 'link_to_root'\n",
    "                    word['deprel'] = 'advcl'\n",
    "                    break\n",
    "\n",
    "\n",
    "## Recalculate the heads:\n",
    "\n",
    "    local_heads = []\n",
    "    verbal_heads = []\n",
    "    all_heads = []   \n",
    "    \n",
    "    \n",
    "    for i,word in enumerate(sentence):                      # Make lists of heads\n",
    "\n",
    "        if word['head'] == 'local_head':\n",
    "            local_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "        elif word['head'] == 'verb_head':\n",
    "            verbal_heads.append(i)\n",
    "            all_heads.append(i)\n",
    "\n",
    "\n",
    "\n",
    "## Find the root\n",
    "    \n",
    "    root = 0\n",
    "                    \n",
    "    if len(all_heads) == 1:                                    # If there is just one possible head, it is the root\n",
    "        sentence[all_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "        root = all_heads[0]\n",
    "\n",
    "    elif len(verbal_heads) == 1:                                 # If there is just one verb head, it is the root (could be better)\n",
    "        sentence[verbal_heads[0]].update({'head' : 0, 'deprel': 'root'})\n",
    "        root = verbal_heads[0]\n",
    "\n",
    "    elif len(verbal_heads) > 1: \n",
    "        sentence[verbal_heads[0]]['head'] == 'root'\n",
    "        root = verbal_heads[0]\n",
    "\n",
    "    root += 1\n",
    "# Mark remaining elements as obliques or adverbial clauses\n",
    "\n",
    "    for word in sentence:\n",
    "        if word['head'] in ('local_head', 'headless_relative'):\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'obl'\n",
    "        elif word['head'] in ('verbal_head', 'link_to_root'):\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'advcl'\n",
    "        elif word['head'] == '_':\n",
    "            word['head'] = root\n",
    "            word['deprel'] = 'adv'\n",
    "        \n",
    "\n",
    "    \n",
    "    return(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(sentence):\n",
    "\n",
    "    for word in sentence:\n",
    "        if 'head' not in word:\n",
    "            word.update({'head' : '_'})\n",
    "        if 'deprel' not in word:\n",
    "            word.update({'deprel' : '_'})\n",
    "        if 'feats' not in word:\n",
    "            word.update({'feats' : '_'})\n",
    "\n",
    "    sentence = nominal_dependency(sentence)\n",
    "\n",
    "    sentence = verbal_dependency(sentence)\n",
    "    \n",
    "    sentence = head_to_head_depedency(sentence)\n",
    "    \n",
    "    \n",
    "    return(sentence)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "bis_dictionary = {}\n",
    "\n",
    "with open(\"../data/BIS_Dictionary_3col.replaced.csv\") as f:\n",
    "    for line in f:\n",
    "        (k, v1, v2) = line.split(',')\n",
    "        if k in bis_dictionary:\n",
    "            bis_dictionary[k].add(v1)\n",
    "        else:\n",
    "            bis_dictionary[k] = {v1}\n",
    "\n",
    "freqdic = {'NOUN': 12, 'VERB' : 11, 'PRON' : 10, 'ADP' : 13, 'PART' : 13, 'ADV' : 4, 'ADJ' : 6, 'NUM': 5, 'CCONJ': 4, 'DET': 3, 'INTJ': 2, 'PROPN': 1, 'AUX': 14}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate over files, extract text and tag text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = \"\"\n",
    "\n",
    "datadir = '../data/BIS_20230512/'\n",
    "\n",
    "dic = bis_dictionary\n",
    "\n",
    "for file in os.listdir(datadir):\n",
    "    filepath = datadir + file\n",
    "    text = Elan2Str.elan2str(filepath, 'default')\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    counter = 0\n",
    "    for sentence in sentences:\n",
    "        counter += 1\n",
    "        metadata = '# ' + 'sent_id = ' + str(file) + '.' + str(counter) + '\\n' + '# ' + 'text = ' + str(sentence) + '\\n' #maybe make these variables \n",
    "        tree += metadata\n",
    "        word_counter = 0\n",
    "        sentence = sentence.strip('. ')                                 #strip here removes final stops\n",
    "        sentence = re.sub(\"\\s*\\%.*?\\%\\s*\", \"\", sentence)                #Removes tags marked with '%'\n",
    "        if not re.match(\"^\\s*[\\,\\.\\(\\)\\{\\}\\[\\]]*\\s*$\", sentence):       #Ignores sentences which are empty or punctuation\n",
    "            token_sentence = nltk.word_tokenize(sentence)               #Tokenise\n",
    "            pos_tagged_sentence = pos_tag(token_sentence, dic)          #POS tag\n",
    "            lemma_tagged_sentence = lemma_tag(pos_tagged_sentence, dic) #Lemmatise\n",
    "            parsed_sentence = parse(lemma_tagged_sentence)              #Depedency parser\n",
    "            for word in parsed_sentence:\n",
    "                word_counter += 1\n",
    "                tree += str(word_counter) + '\\t' + word['word'] + '\\t' + word['lemma'] + '\\t' + word['pos'] + '\\t' + '_' + '\\t' +  '_' + '\\t' + str(word['head']) + '\\t' + word['deprel'] + '\\t' + '_' + '\\t' + '_' + '\\n'    \n",
    "        tree += '\\n'\n",
    "        \n",
    "        \n",
    "with open('Bislama_Tree.conllu', 'w') as output:\n",
    "    output.write(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
